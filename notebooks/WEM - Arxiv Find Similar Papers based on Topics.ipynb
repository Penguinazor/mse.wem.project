{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"arxiv_data_cs_all.pickle.bz2\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: urllib3\r\n",
      "Version: 1.24.1\r\n",
      "Summary: HTTP library with thread-safe connection pooling, file post, and more.\r\n",
      "Home-page: https://urllib3.readthedocs.io/\r\n",
      "Author: Andrey Petrov\r\n",
      "Author-email: andrey.petrov@shazow.net\r\n",
      "License: MIT\r\n",
      "Location: /home/rclaret/anaconda3/envs/py36/lib/python3.6/site-packages\r\n",
      "Requires: \r\n",
      "Required-by: requests, botocore\r\n"
     ]
    }
   ],
   "source": [
    "!pip show urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks for the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.\n",
      "Andy Coenen\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[11,4])\n",
    "print(df.iloc[11,5][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min = 2 #100\n",
    "df_max = 0.1 #0.2\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "cvec = CountVectorizer(min_df=df_min, max_df=df_max, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "trans = cvec.fit_transform(df.summary.tolist())\n",
    "\n",
    "corpus = Sparse2Corpus(trans, documents_columns=False)\n",
    "\n",
    "id_map = dict((v, k) for k, v in cvec.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks for the LDA Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66987\n",
      "ignoring\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(list(id_map)))\n",
    "print(list(cvec.vocabulary_.keys())[11])\n",
    "print(\"lda\" in cvec.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "n_topics = 10\n",
    "n_passes = 50\n",
    "\n",
    "model_path = \"lda/arxiv_data_cs_all_ntopics_\"+str(n_topics)+\"_npasses_\"+str(n_passes)+\"_nvoc_\"+str(len(list(id_map)))+\"_dfmin_\"+str(df_min)+\"_dfmax_\"+str(int(df_max*100))+\".model\"\n",
    "ldamodel = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks for the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"distribution\" + 0.009*\"probability\" + 0.009*\"random\" + 0.009*\"function\" + 0.007*\"error\"'),\n",
       " (1,\n",
       "  '0.014*\"channel\" + 0.011*\"power\" + 0.011*\"energy\" + 0.009*\"rate\" + 0.008*\"scheme\"'),\n",
       " (2,\n",
       "  '0.015*\"matrix\" + 0.010*\"sparse\" + 0.008*\"signal\" + 0.008*\"dimensional\" + 0.007*\"space\"'),\n",
       " (3,\n",
       "  '0.011*\"logic\" + 0.010*\"quantum\" + 0.009*\"theory\" + 0.006*\"properties\" + 0.006*\"complexity\"'),\n",
       " (4,\n",
       "  '0.015*\"classification\" + 0.014*\"training\" + 0.012*\"features\" + 0.010*\"neural\" + 0.009*\"feature\"'),\n",
       " (5,\n",
       "  '0.022*\"graph\" + 0.014*\"graphs\" + 0.013*\"bound\" + 0.013*\"codes\" + 0.010*\"bounds\"'),\n",
       " (6,\n",
       "  '0.011*\"social\" + 0.009*\"research\" + 0.008*\"users\" + 0.007*\"user\" + 0.005*\"web\"'),\n",
       " (7,\n",
       "  '0.012*\"software\" + 0.008*\"search\" + 0.008*\"computing\" + 0.007*\"implementation\" + 0.007*\"design\"'),\n",
       " (8,\n",
       "  '0.020*\"control\" + 0.012*\"game\" + 0.010*\"agents\" + 0.009*\"games\" + 0.008*\"agent\"'),\n",
       " (9,\n",
       "  '0.030*\"image\" + 0.018*\"images\" + 0.013*\"detection\" + 0.011*\"video\" + 0.010*\"object\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=n_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abtract to get similarities from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_abstract = \"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.\"\n",
    "\n",
    "#text_abstract = \"Argus exploits a Multi-Agent Reinforcement Learning (MARL) framework to create a 3D mapping of the disaster scene using agents present around the incident zone to facilitate the rescue operations. The agents can be both human bystanders at the disaster scene as well as drones or robots that can assist the humans. The agents are involved in capturing the images of the scene using their smartphones (or on-board cameras in case of drones) as directed by the MARL algorithm. These images are used to build real time a 3D map of the disaster scene. Via both simulations and real experiments, an evaluation of the framework in terms of effectiveness in tracking random dynamicity of the environment is presented.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get related topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import Sparse2Corpus\n",
    "import numpy as np\n",
    "\n",
    "def get_similar_topics_distribution(abst_to_match):\n",
    "    topics_array = np.zeros(n_topics)\n",
    "    \n",
    "    trans = cvec.transform(list([abst_to_match])) \n",
    "    corpus = Sparse2Corpus(trans, documents_columns=False)\n",
    "    results = list(ldamodel.get_document_topics(bow=corpus))[0]\n",
    "    \n",
    "    for items in results:\n",
    "        topics_array[items[0]] = items[1]\n",
    "    return topics_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27 (4, '0.015*\"classification\" + 0.014*\"training\" + 0.012*\"features\" + 0.010*\"neural\" + 0.009*\"feature\"') \n",
      "\n",
      "0.26 (0, '0.010*\"distribution\" + 0.009*\"probability\" + 0.009*\"random\" + 0.009*\"function\" + 0.007*\"error\"') \n",
      "\n",
      "0.18 (2, '0.015*\"matrix\" + 0.010*\"sparse\" + 0.008*\"signal\" + 0.008*\"dimensional\" + 0.007*\"space\"') \n",
      "\n",
      "0.14 (7, '0.012*\"software\" + 0.008*\"search\" + 0.008*\"computing\" + 0.007*\"implementation\" + 0.007*\"design\"') \n",
      "\n",
      "0.1 (5, '0.022*\"graph\" + 0.014*\"graphs\" + 0.013*\"bound\" + 0.013*\"codes\" + 0.010*\"bounds\"') \n",
      "\n",
      "0.04 (1, '0.014*\"channel\" + 0.011*\"power\" + 0.011*\"energy\" + 0.009*\"rate\" + 0.008*\"scheme\"') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "lda_topics_with_5_words = ldamodel.print_topics(num_topics=n_topics, num_words=5)\n",
    "#lda_topics_with_5_words = ldamodel.get_topic_terms(6, topn=10)\n",
    "\n",
    "for i, item in enumerate(get_similar_topics_distribution(text_abstract)):\n",
    "    if item > 0:\n",
    "        results.append([i,item])\n",
    "\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#print(results)\n",
    "\n",
    "for r in results:\n",
    "    print(round(r[1],2), lda_topics_with_5_words[r[0]],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Papers Matrix Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers_path = \"pickles/arxiv_data_cs_all_ntopics_\"+str(n_topics)+\"_npasses_\"+str(n_passes)+\"_nvoc_\"+str(len(list(id_map)))+\"_dfmin_\"+str(df_min)+\"_dfmax_\"+str(int(df_max*100))+\"_paperdf.pickle.bz2\"\n",
    "df_papers = pd.read_pickle(df_papers_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks Papers Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "      <td>180644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.106808</td>\n",
       "      <td>0.117729</td>\n",
       "      <td>0.086715</td>\n",
       "      <td>0.096656</td>\n",
       "      <td>0.102931</td>\n",
       "      <td>0.116628</td>\n",
       "      <td>0.119399</td>\n",
       "      <td>0.105897</td>\n",
       "      <td>0.068867</td>\n",
       "      <td>0.069535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.153645</td>\n",
       "      <td>0.209588</td>\n",
       "      <td>0.145671</td>\n",
       "      <td>0.176655</td>\n",
       "      <td>0.181113</td>\n",
       "      <td>0.197873</td>\n",
       "      <td>0.185187</td>\n",
       "      <td>0.153533</td>\n",
       "      <td>0.121295</td>\n",
       "      <td>0.145008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.035985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018142</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.033596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.160989</td>\n",
       "      <td>0.129646</td>\n",
       "      <td>0.112872</td>\n",
       "      <td>0.105934</td>\n",
       "      <td>0.127626</td>\n",
       "      <td>0.141316</td>\n",
       "      <td>0.165402</td>\n",
       "      <td>0.158722</td>\n",
       "      <td>0.084873</td>\n",
       "      <td>0.061561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.989652</td>\n",
       "      <td>0.992241</td>\n",
       "      <td>0.983632</td>\n",
       "      <td>0.986153</td>\n",
       "      <td>0.990321</td>\n",
       "      <td>0.990623</td>\n",
       "      <td>0.982690</td>\n",
       "      <td>0.969994</td>\n",
       "      <td>0.963993</td>\n",
       "      <td>0.988155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1              2              3  \\\n",
       "count  180644.000000  180644.000000  180644.000000  180644.000000   \n",
       "mean        0.106808       0.117729       0.086715       0.096656   \n",
       "std         0.153645       0.209588       0.145671       0.176655   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.035985       0.000000       0.015821       0.000000   \n",
       "75%         0.160989       0.129646       0.112872       0.105934   \n",
       "max         0.989652       0.992241       0.983632       0.986153   \n",
       "\n",
       "                   4              5              6              7  \\\n",
       "count  180644.000000  180644.000000  180644.000000  180644.000000   \n",
       "mean        0.102931       0.116628       0.119399       0.105897   \n",
       "std         0.181113       0.197873       0.185187       0.153533   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.018142       0.026214       0.033596   \n",
       "75%         0.127626       0.141316       0.165402       0.158722   \n",
       "max         0.990321       0.990623       0.982690       0.969994   \n",
       "\n",
       "                   8              9  \n",
       "count  180644.000000  180644.000000  \n",
       "mean        0.068867       0.069535  \n",
       "std         0.121295       0.145008  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.084873       0.061561  \n",
       "max         0.963993       0.988155  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.063365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612595</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.341914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1         2    3         4         5         6         7    8  \\\n",
       "0  0.0  0.0  0.080399  0.0  0.249420  0.063365  0.000000  0.000000  0.0   \n",
       "1  0.0  0.0  0.000000  0.0  0.612595  0.023915  0.011652  0.341914  0.0   \n",
       "\n",
       "          9  \n",
       "0  0.599737  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 2, 7, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "results_topics = []\n",
    "for r in results:\n",
    "    results_topics.append(r[0])\n",
    "print(results_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter none related rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal dataset size 180644\n",
      "Filtered dataset size 2708\n"
     ]
    }
   ],
   "source": [
    "df_filtered_papers = df_papers.copy()\n",
    "print(\"Orginal dataset size\",len(df_filtered_papers))\n",
    "\n",
    "for r in results:\n",
    "    df_filtered_papers = df_filtered_papers.where(df_filtered_papers[r[0]]>0).dropna()\n",
    "print(\"Filtered dataset size\",len(df_filtered_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting the related rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size 2708\n",
      "Weighted dataset size 15\n"
     ]
    }
   ],
   "source": [
    "df_weighted_papers_filtered = df_filtered_papers.copy()\n",
    "print(\"Filtered dataset size\",len(df_weighted_papers_filtered))\n",
    "for i in range(len(results)-1):\n",
    "    df_weighted_papers_filtered = df_weighted_papers_filtered.where(df_weighted_papers_filtered[results[i][0]]>df_weighted_papers_filtered[results[i+1][0]]).dropna()\n",
    "\n",
    "print(\"Weighted dataset size\",len(df_weighted_papers_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_test_papers_filtered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5245,   8723,  10651,  12922,  26187,  36022,  38129,  47580,\n",
       "        73065,  73689,  73767,  90315, 102916, 110867, 141588])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_papers_index = df_test_papers_filtered.index.values.astype(int)\n",
    "similar_papers_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5245, 'http://arxiv.org/abs/1901.03706v5'],\n",
       " [8723, 'http://arxiv.org/abs/1904.12933v1'],\n",
       " [10651, 'http://arxiv.org/abs/1904.08688v1'],\n",
       " [12922, 'http://arxiv.org/abs/1811.09341v2'],\n",
       " [26187, 'http://arxiv.org/abs/1806.07366v4'],\n",
       " [36022, 'http://arxiv.org/abs/1811.00894v1'],\n",
       " [38129, 'http://arxiv.org/abs/1810.07491v1'],\n",
       " [47580, 'http://arxiv.org/abs/0911.4983v1'],\n",
       " [73065, 'http://arxiv.org/abs/1708.05464v1'],\n",
       " [73689, 'http://arxiv.org/abs/1708.03052v1'],\n",
       " [73767, 'http://arxiv.org/abs/1704.02124v2'],\n",
       " [90315, 'http://arxiv.org/abs/1612.08882v2'],\n",
       " [102916, 'http://arxiv.org/abs/1606.07369v1'],\n",
       " [110867, 'http://arxiv.org/abs/1511.05078v2'],\n",
       " [141588, 'http://arxiv.org/abs/1507.01826v2']]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_papers_by_index = []\n",
    "for i in similar_papers_index:\n",
    "    similar_papers_by_index.append([i,df.iloc[i,0]])\n",
    "similar_papers_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73689, 'http://arxiv.org/abs/1708.03052v1']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_papers_by_index[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank retrived articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(abstract_1, abstract_2):\n",
    "    return np.dot(get_similar_topics_distribution(abstract_1),\n",
    "                  get_similar_topics_distribution(abstract_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_papers_summaries = []\n",
    "for i in similar_papers_index:\n",
    "    similar_papers_summaries.append(df.iloc[i,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me at: 4\n",
      "[[0, 0.2545948518225215], [9, 0.2156067581129495], [5, 0.20817100388195325], [3, 0.20246628537173106], [10, 0.19847078589184258], [6, 0.19455812937375738], [1, 0.17897905241003836], [11, 0.1786961457787899], [2, 0.17338374635551135], [12, 0.16507263437871061], [14, 0.1537525284028094], [8, 0.10652350212058317], [7, 0.06271363104310562], [13, 0.04201339097344092]]\n"
     ]
    }
   ],
   "source": [
    "similar_papers_ranked_by_id = []\n",
    "for i, abstract in enumerate(similar_papers_summaries):\n",
    "    if text_abstract == abstract:\n",
    "            print(\"me at:\",i)\n",
    "            continue\n",
    "    similar_papers_ranked_by_id.append([i, get_score(text_abstract, abstract)])\n",
    "\n",
    "similar_papers_ranked_by_id = sorted(similar_papers_ranked_by_id, key=lambda x: x[1], reverse=True)\n",
    "print(similar_papers_ranked_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the ranked similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5245, 'http://arxiv.org/abs/1901.03706v5'],\n",
       " [73689, 'http://arxiv.org/abs/1708.03052v1'],\n",
       " [36022, 'http://arxiv.org/abs/1811.00894v1'],\n",
       " [12922, 'http://arxiv.org/abs/1811.09341v2'],\n",
       " [73767, 'http://arxiv.org/abs/1704.02124v2'],\n",
       " [38129, 'http://arxiv.org/abs/1810.07491v1'],\n",
       " [8723, 'http://arxiv.org/abs/1904.12933v1'],\n",
       " [90315, 'http://arxiv.org/abs/1612.08882v2'],\n",
       " [10651, 'http://arxiv.org/abs/1904.08688v1'],\n",
       " [102916, 'http://arxiv.org/abs/1606.07369v1'],\n",
       " [141588, 'http://arxiv.org/abs/1507.01826v2'],\n",
       " [73065, 'http://arxiv.org/abs/1708.05464v1'],\n",
       " [47580, 'http://arxiv.org/abs/0911.4983v1'],\n",
       " [110867, 'http://arxiv.org/abs/1511.05078v2']]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_papers = []\n",
    "for r in similar_papers_ranked_by_id:\n",
    "    ranked_papers.append(similar_papers_by_index[r[0]])\n",
    "ranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.\""
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summary[similar_papers_ranked_by_id[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
