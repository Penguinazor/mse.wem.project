{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Crawler\n",
    "## Limitation\n",
    "https://cran.r-project.org/web/packages/aRxiv/vignettes/aRxiv.html\n",
    "- maximum 50k records per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "XML_ENTRIES = \"{http://www.w3.org/2005/Atom}entry\"\n",
    "XML_ID = \"{http://www.w3.org/2005/Atom}id\"\n",
    "XML_UPDATED = \"{http://www.w3.org/2005/Atom}updated\"\n",
    "XML_PUBLISHED = \"{http://www.w3.org/2005/Atom}published\"\n",
    "XML_TITLE = \"{http://www.w3.org/2005/Atom}title\"\n",
    "XML_SUMMARY = \"{http://www.w3.org/2005/Atom}summary\"\n",
    "\n",
    "XML_AUTHOR = \"{http://www.w3.org/2005/Atom}author\"\n",
    "XML_AUTHOR_NAME = \"{http://www.w3.org/2005/Atom}name\"\n",
    "XML_AUTHOR_AFFILIATION = \"{http://arxiv.org/schemas/atom}affiliation\"\n",
    "\n",
    "XML_DOI = \"{http://arxiv.org/schemas/atom}doi\"\n",
    "XML_JOURNAL_REF = \"{http://arxiv.org/schemas/atom}journal_ref\"\n",
    "XML_LINKS = \"{http://www.w3.org/2005/Atom}link\"\n",
    "\n",
    "XML_PRIMARY_CATEGORY = \"{http://arxiv.org/schemas/atom}primary_category\"\n",
    "XML_CATEGORY = \"{http://www.w3.org/2005/Atom}category\"\n",
    "\n",
    "\n",
    "def parse_to_list(parsed):\n",
    "    entries_list = []\n",
    "    entries_root = parsed.findall(\"./\"+XML_ENTRIES)\n",
    "    \n",
    "    if parsed.find(\"./\"+XML_ENTRIES):\n",
    "        for entry in entries_root:        \n",
    "            entry_list = []\n",
    "\n",
    "            entry_list.append(entry.find(\"./\"+XML_ID).text) # Entry ID\n",
    "            entry_list.append(entry.find(\"./\"+XML_UPDATED).text) # Entry Update Date\n",
    "            entry_list.append(entry.find(\"./\"+XML_PUBLISHED).text) # Entry Published Date\n",
    "            entry_list.append(entry.find(\"./\"+XML_TITLE).text) # Entry Title\n",
    "            entry_list.append(entry.find(\"./\"+XML_SUMMARY).text) # Entry Summary\n",
    "\n",
    "            # List of entry authors and their affiliation is any\n",
    "            authors_name_list = []\n",
    "            authors_affiliation_list = []\n",
    "            for author in entry.findall(\"./\"+XML_AUTHOR):\n",
    "                authors_name_list.append(author.find(\"./\"+XML_AUTHOR_NAME).text)\n",
    "                if author.find(\"./\"+XML_AUTHOR_AFFILIATION) != None:\n",
    "                    authors_affiliation_list.append(author.find(\"./\"+XML_AUTHOR_AFFILIATION).text)\n",
    "            entry_list.append(authors_name_list)\n",
    "            entry_list.append(authors_affiliation_list)\n",
    "\n",
    "            # Entry DOI\n",
    "            doi = entry.find(\"./\"+XML_DOI)\n",
    "            if doi is not None:\n",
    "                entry_list.append(doi.text)\n",
    "            else:\n",
    "                entry_list.append(None)\n",
    "\n",
    "            # Entry Journal Ref\n",
    "            jref = entry.find(\"./\"+XML_JOURNAL_REF)\n",
    "            if jref is not None:\n",
    "                entry_list.append(jref.text)\n",
    "            else:\n",
    "                entry_list.append(None)\n",
    "\n",
    "            # Entry PDF link\n",
    "            for links in entry.findall(\"./\"+XML_LINKS):\n",
    "                if (links.attrib.get(\"title\") == \"pdf\"): entry_list.append(links.attrib.get(\"href\"))\n",
    "\n",
    "            # Entry primary category\n",
    "            entry_list.append(entry.find(\"./\"+XML_PRIMARY_CATEGORY).attrib.get(\"term\"))\n",
    "\n",
    "            # Entry Categories\n",
    "            authors_categories_list = []\n",
    "            for category in entry.findall(\"./\"+XML_CATEGORY):\n",
    "                for term in category.attrib.get(\"term\").split(';'):\n",
    "                    authors_categories_list.append(term.strip())\n",
    "            entry_list.append(authors_categories_list)\n",
    "\n",
    "            # Build entries list\n",
    "            entries_list.append(entry_list) # Append to main entry list\n",
    "            \n",
    "        return entries_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53022"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries_list = backup_entries_list\n",
    "len(entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Downloading chunk 2 from 26.0 with 1000 elements s \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=1000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "\t>>> Downloading chunk 3 from 26.0 with 2000 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=2000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "Try: 2/10\n",
      "\t>>> Downloading chunk 5 from 26.0 with 4000 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=4000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "\t>>> Downloading chunk 7 from 26.0 with 6000 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=6000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "\t>>> Downloading chunk 10 from 26.0 with 8400 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=9000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "Try: 2/10\n",
      "\t>>> Downloading chunk 13 from 26.0 with 11400 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=12000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "\t>>> Downloading chunk 15 from 26.0 with 13400 elements \tNO ENTRIES-> TRYING AGAIN\n",
      "http://export.arxiv.org/api/query?search_query=cat:cs*+AND+submittedDate:[201001010000+20120905175052TO+]&start=14000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending\n",
      "Try: 1/10\n",
      "Try: 2/10\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/help/api/index\n",
    "# https://arxiv.org/help/api/user-manual\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "import socket\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "#http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=1\n",
    "#http://export.arxiv.org/api/query?search_query=cat:cs.LG&sortBy=submittedDate&sortOrder=descending&&start='+str(n_start)+'&max_results='+str(max_articles_per_attempt)\n",
    "\n",
    "date_start = \"201001010000\"\n",
    "date_end = \"20120905175052\"#\"20151226190200\"#\"20170909215812\" #20180712194129\n",
    "base_url = 'http://export.arxiv.org/api/query?search_query='\n",
    "cat_type = \"cat\"\n",
    "categroy = cat_type+\":cs\" #all:\n",
    "subject = \"*+AND+submittedDate:[\"+date_start+\"+\"+date_end+\"TO+]\"#\"*+AND+submittedDate:[201001010000+TO+20180712194129]\" #\"deep\" # \"\\\"electron thermal conductivity\\\"\"\n",
    "sort_by=\"lastUpdatedDate\"\n",
    "sort_order=\"descending\"#\"ascending\"\n",
    "\n",
    "#2018-07-12T19:41:20Z\n",
    "\n",
    "start_value = 0#len(entries_list) #0\n",
    "max_results = 1000\n",
    "n_attempts = 999999999\n",
    "max_articles_per_attempt = 1000\n",
    "max_tries = 10\n",
    "sleep_time = 5\n",
    "socket.setdefaulttimeout(120.0)\n",
    "\n",
    "if start_value <= 0:\n",
    "    entries_list = []\n",
    "start_time = time.time()\n",
    "for i in range(n_attempts):\n",
    "    print(\"\\r\\t>>> Downloading chunk {} from {} with {} elements\".format(i+1, n_attempts,len(entries_list)), end=' ')\n",
    "    \n",
    "    n_start = start_value+i*max_articles_per_attempt\n",
    "        \n",
    "    url = (base_url + categroy + subject +\n",
    "           \"&start=\" + str(n_start) +\n",
    "           \"&max_results=\" + str(max_articles_per_attempt) +\n",
    "           \"&sortBy=\" + sort_by +\n",
    "           \"&sortOrder\" + sort_order)\n",
    "    \n",
    "    data = urlopen(url, None, timeout=120).read()\n",
    "    parsed = ET.fromstring(data)\n",
    "    \n",
    "    \n",
    "    if i==0:\n",
    "        total_results = int(parsed.find(\"{http://a9.com/-/spec/opensearch/1.1/}totalResults\").text)\n",
    "        if total_results == 0:\n",
    "            print(\"\\tNO RESULTS-> ABORT NOW\")\n",
    "            print(url)\n",
    "            break\n",
    "        else:\n",
    "            n_attempts = ceil(total_results/max_articles_per_attempt)-len(entries_list)/max_articles_per_attempt\n",
    "    \n",
    "    if parsed.find(\"./\"+XML_ENTRIES):\n",
    "        entries_list+=parse_to_list(parsed)\n",
    "    else:\n",
    "        print(\"\\tNO ENTRIES-> TRYING AGAIN\")\n",
    "        print(url)\n",
    "        for j in range(max_tries):\n",
    "            print(\"Try: \"+str(j+1)+\"/\"+str(max_tries))\n",
    "            time.sleep(10)\n",
    "            data = urlopen(url, None, timeout=120).read()\n",
    "            parsed = ET.fromstring(data)\n",
    "            if parsed.find(\"./\"+XML_ENTRIES):\n",
    "                entries_list+=parse_to_list(parsed)\n",
    "                break\n",
    "                \n",
    "    if not parsed.find(\"./\"+XML_ENTRIES):\n",
    "        print(\"\\tTERMINATED\")\n",
    "        print(url)\n",
    "        break \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/1209.1060v11',\n",
       " '2014-08-03T13:51:49Z',\n",
       " '2012-09-05T17:50:52Z',\n",
       " 'Combinatorial Spaces And Order Topologies',\n",
       " '  An archetypal problem discussed in computer science is the problem of\\nsearching for a given number in a given set of numbers. Other than sequential\\nsearch, the classic solution is to sort the list of numbers and then apply\\nbinary search. The binary search problem has a complexity of O(logN) for a list\\nof N numbers while the sorting problem cannot be better than O(N) on any\\nsequential computer following the usual assumptions. Whenever the problem of\\ndeciding partial order can be done in O(1), a variation of the problem on some\\nbounded list of numbers is to apply binary search without resorting to sort.\\nThe overall complexity of the problem is then O(log R) for some radius R. A\\nlogarithmic upper-bound for finite encodings is shown. Also, the topology of\\norderings can provide efficient algorithms for search problems in combinatorial\\nspaces. The main characteristic of those spaces is that they have typical\\nexponential space complexities. The factorial case describes an order topology\\nthat can be illustrated using the combinatorial polytope . When a known order\\ntopology can be combined to a given formulation of a search problem, the\\nresulting search problem has a polylogarithmic complexity. This logarithmic\\ncomplexity can then become useful in combinatorial search by providing a\\nlogarithmic break-down. These algorithms can be termed as the class of search\\nalgorithms that do not require read and are equivalent to the class of\\nlogarithmically recursive functions. Also, the notion of order invariance is\\ndiscussed.\\n',\n",
       " ['Philon Nguyen'],\n",
       " [],\n",
       " None,\n",
       " None,\n",
       " 'http://arxiv.org/pdf/1209.1060v11',\n",
       " 'cs.CC',\n",
       " ['cs.CC', 'cs.DM']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "backup_entries_list = deepcopy(entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49600"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backup_entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://export.arxiv.org/api/query?search_query=cat:cs*&start=5000&max_results=1000&sortBy=lastUpdatedDate&sortOrderdescending'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ['id', 'updated', 'published', 'title', 'summary', 'authors', 'affiliations', 'doi', 'journal_ref', 'pdf_link', 'primary_category', 'categories']\n",
    "\n",
    "def sanitize_str(s):\n",
    "    if s:\n",
    "        s = re.sub('[\\r\\n]+', ' ', s)\n",
    "        s = re.sub(' +', ' ', s)\n",
    "        s = re.sub('\\$(.|\\n)+?\\$', '', s)\n",
    "        s = re.sub('\\$\\$(.|\\n)+?\\$\\$', '', s)\n",
    "        s = re.sub('\\(', '', s)\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "        \n",
    "        return s.strip()\n",
    "    return s\n",
    "\n",
    "def create_df(entries):\n",
    "    df = pd.DataFrame(entries_list, columns=HEADERS)\n",
    "    df['authors'] = df['authors'].apply(lambda x: '|'.join(x))\n",
    "    df['affiliations'] = df['affiliations'].apply(lambda x: '|'.join(x))\n",
    "    df['categories'] = df['categories'].apply(lambda x: '|'.join(x))\n",
    "    return df.applymap(sanitize_str)\n",
    "\n",
    "def write_df(entries_df, filename):\n",
    "    filename = filename+'.csv.bz2'\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print('Creating file: '+filename)\n",
    "        entries_df.to_csv(filename, index=False, compression='bz2', mode='w')\n",
    "    else:\n",
    "        print('Writing file: '+filename)\n",
    "        entries_df.to_csv(filename, index=False, compression='bz2', mode='a', header=False)\n",
    "        \n",
    "    print('Done!')\n",
    "    \n",
    "def pickle_data(entries, filename):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    filename = \"pickles/\"+filename+'.pickle.bz2'\n",
    "    df = pd.DataFrame(entries_list, columns=HEADERS)\n",
    "    df.summary = df.summary.map(sanitize_str)\n",
    "    print(\"Done!\",filename)\n",
    "    return df.to_pickle(filename, compression='bz2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file: arxiv_data_1560013031.csv.bz2\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ARXIV_DATA_FILENAME = 'arxiv_data_'+str(int(time.time()))\n",
    "\n",
    "df = create_df(entries_list)\n",
    "write_df(df, ARXIV_DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! pickles/arxiv_data_cat_201001010000_to_20151226190200_1560070379.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "dates = date_start+\"_to_\"+date_end\n",
    "ARXIV_DATA_PICKLE = \"arxiv_data_\"+cat_type+\"_\"+dates+\"_\"+str(int(time.time()))\n",
    "\n",
    "pickle_data(entries_list, ARXIV_DATA_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
